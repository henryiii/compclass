{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Day 1: (Pro)file\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Cover profiling (extension of last lecture)\n",
    "* Cover reading and writing of files\n",
    "* Start the basics of ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "Quick extension of last week's lecture: Remember we said that you should optimize only the slow parts of your program. How do you tell? Profile!\n",
    "\n",
    "There are two types of profilers:\n",
    "1. Deterministic profilers: These modify the runtime of the program by tracking every line.\n",
    "2. Sampling profilers: These \"sample\" every so often.\n",
    "\n",
    "The main profiler for Python is the built in `cProfile` and `profile` modules (the first being a faster version of the second). Since Python is already interpreted and slow, most profilers for Python are deterministic profilers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses the built in profiler directly, as well as `line_profiler`, from either conda (`conda install line_profiler` in environment) or pip (`pip install --user line-profiler`). Line profiler is a powerful, better interface (and IPython extension) for the cProfile module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(data, samples=4, mu_init=.5, proposal_width=.5, plot=False, mu_prior_mu=0, mu_prior_sd=1.):\n",
    "    mu_current = mu_init\n",
    "    posterior = [mu_current]\n",
    "    for i in range(samples):\n",
    "        # suggest new position\n",
    "        mu_proposal = norm(mu_current, proposal_width).rvs()\n",
    "\n",
    "        # Compute likelihood by multiplying probabilities of each data point\n",
    "        likelihood_current = norm(mu_current, 1).pdf(data).prod()\n",
    "        likelihood_proposal = norm(mu_proposal, 1).pdf(data).prod()\n",
    "        \n",
    "        # Compute prior probability of current and proposed mu        \n",
    "        prior_current = norm(mu_prior_mu, mu_prior_sd).pdf(mu_current)\n",
    "        prior_proposal = norm(mu_prior_mu, mu_prior_sd).pdf(mu_proposal)\n",
    "        \n",
    "        p_current = likelihood_current * prior_current\n",
    "        p_proposal = likelihood_proposal * prior_proposal\n",
    "        \n",
    "        # Accept proposal?\n",
    "        p_accept = p_proposal / p_current\n",
    "        \n",
    "        # Usually would include prior probability, which we neglect here for simplicity\n",
    "        accept = np.random.rand() < p_accept\n",
    "        \n",
    "        if plot:\n",
    "            plot_proposal(mu_current, mu_proposal, mu_prior_mu, mu_prior_sd, data, accept, posterior, i)\n",
    "        \n",
    "        if accept:\n",
    "            # Update position\n",
    "            mu_current = mu_proposal\n",
    "        \n",
    "        posterior.append(mu_current)\n",
    "        \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "data = np.random.randn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('posterior = sampler(data, samples=1000, mu_init=1.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading that last page is a bit tricky - we have the \"insides\" of all the Python functions being called. Let's try `line_profiler` to just select the *one* function we care about, and see that line-by-line.\n",
    "\n",
    "Like all external packages except matplotlib, we need to load the package as an IPython extension first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the `lprun` magic along with the `-f function` to select a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f sampler posterior = sampler(data, samples=1000, mu_init=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! I can easily see what's wrong now. This allows you to *target* optimizations; often minor changes, like `np.stack` instead of `np.array`, may give you large speedups. Parts that are not performance critical can remain clear and easy to read/maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real life example:\n",
    "* I had 128 DataFrames of around a million events each, and I wanted to make 128 2D histograms.\n",
    "* Custom Histogram library Physt was taking 3 seconds per histogram.\n",
    "* Native Numpy histogram (then feeding that into Physt) took .9 seconds per histogram.\n",
    "* Custom Numba histogram function (then feeding that into Physt) took 0.1 seconds per histogram.\n",
    "\n",
    "At this point, a total of 10-15 seconds for the procedure was fine. Physt histograms as output means I can use the nice OO design to merge and rebin histograms later, by the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "\n",
    "Let's look at reading in and out data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic input/output\n",
    "\n",
    "As a reminder, this is input output of text or data files in pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmpfile.txt', 'w') as f:\n",
    "    f.write(\"This is one line\\n\")\n",
    "    print(\"Second line\", file=f)\n",
    "# File gets closed here\n",
    "\n",
    "with open('tmpfile.txt') as f:\n",
    "    for l in f:\n",
    "        print(l, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over a file to get lines, or use `.readlines` to get a list of lines, or `.read` to just read in (part) of the file. The mode flags can set text vs. binary `'b'`, read `'r'`, write `'w'`, or append `'a'`.\n",
    "\n",
    "The `f` object is a File Buffer. Quite a few functions and libraries in Python take a buffer - usually they take a file name or an open buffer. You can make a buffer that is attached to your memory instead of a file with `io.StringIO`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing any objects\n",
    "\n",
    "Python has a protocol for storing and recovering almost any object called \"pickling\". This is general purpose, but is often not ideal for large or long term storage, or for communicating data - there is some limitation on Python version, the class code should be similar or the same, and there is no compression. But, it can be handy in a pinch. You can use `dump` and `load`, or you can use `dumps` and `loads` to input/output directly to a string instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_str = pickle.dumps(\"some object\")\n",
    "print(pickle_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.loads(pickle_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: configuration\n",
    "\n",
    "Let's look at better ways to do IO for specific cases. Let's look at a common one: storing some \"configuration\" like data (or any small set of data). As long as the data is small, it's nice to leave it human readable format, in a text file. Here are some popular formats:\n",
    "\n",
    "1. **XML**: Famous - HTML is a subset of XML. Very verbose and ugly, and often used with a custom schema.\n",
    "2. **JSON**: Popular. A little unfriendly for writing, but clean and simple. A subset of JavaScript makes it web-friendly. Great libraries available.\n",
    "3. **INI**: Not well defined, and *very* limited. Very easy for a human to author, though.\n",
    "4. **YAML**: Yet Another Markup Language, popular in some areas. Lots of weird corner cases to the syntax, though.\n",
    "5. **TOML**: Simpler version of the thing above. Gaining some use in Python as of late.\n",
    "\n",
    "Of these, the best standard library support in Python goes to JSON, so we'll look at that one. It's the best in the list for (small amounts of) general data. For configuration, it's a tossup.\n",
    "\n",
    "JSON has the same methods as pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_to_save = [\"list\",'items',{\"dictionary\":\"Nested\", \"number\":2}]\n",
    "\n",
    "print(json.dumps(structure_to_save, indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more at [Python 3 Module of the Week](https://pymotw.com/3/index.html): [json](https://pymotw.com/3/json/index.html>) or the [official documentation](https://docs.python.org/3/library/json.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger data storage\n",
    "#### Numpy\n",
    "Let's assume we have data in Numpy. We can use one of several Numpy methods to store the data; we can even compress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is a simple binary one-array-per-file format; you can use `savez` to save several files into one uncompressed zip file, or `savez_compressed` to save to a compressed zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('tmp.npz', a=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('tmp.npz') as f:\n",
    "    print(f['a'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple, this is still Numpy specific, and is not ideal for cross-language data storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDF5\n",
    "\n",
    "This is a standard format for storing binary structured data with optional compression. It has several nice features, like groups and attributes, that make it very powerful. You need an external library, but Anaconda comes with it by default. Technically, it comes with two. H5py and PyTables; I use HDF5 in C++ also, so I like the fact that H5py looks more like other HDF5 libraries, so I'll cover that. The other one is available with `import tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('tmp.h5', 'w') as f:\n",
    "    f['a'] = a\n",
    "\n",
    "with h5py.File('tmp.h5') as f:\n",
    "    b = f['a'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common idiom in modern IO libraries; you treat the file like a dictionary. There are explicit methods if you need to do something fancier (like turn on compression for a dataset). HDF5 has tools to allow you to keep a dataset on file instead of reading it entirely to memory (but I've not had to use those). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there are great non-Python tools for HDF5 available, like a graphical file viewer.\n",
    "\n",
    "<font color=\"grey\">\n",
    "\n",
    "> Note for HEP physicists: the ROOT file format has a few features that HDF5 does not, like incremental saving. However, other than small differences, the formats are similar. Except that HDF5 is available everywhere and even if it isn't, it takes something like 5 minutes to build instead of several hours..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "Pandas has very powerful connectors to lots of different formats, including CSV, JSON, Excel, and HDF5. If your data is already a table, just use Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20130101',periods=6)\n",
    "df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('tmp.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf('tmp.h5', 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Uproot\n",
    "\n",
    "In HEP, we have a very commonly used and powerful format: ROOT. But getting ROOT to compile is a long (2+ hour) ordeal, and I have yet to get it to work with Anaconda Python on macOS. \n",
    "\n",
    "An exciting new library, \"uproot\" lets you read ROOT files in pure Python without ROOT. The original name was to be micro-root, or $\\mu$root, but it became uproot. You've already seen a bit of uproot in the previous lectures.\n",
    "\n",
    "Writing files is planned, but still in progress."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mimic OSC",
   "language": "python",
   "name": "sys_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
