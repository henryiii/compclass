---
redirect_from:
  - "/week14/1-graphs"
interact_link: content/week14/1_graphs.ipynb
kernel_name: compclass
kernel_path: content/week14
has_widgets: false
title: |-
  Static Computation Graphs
pagenum: 43
prev_page:
  url: /week13/1_review.html
next_page:
  url: /week14/2_ml.html
suffix: .ipynb
search: numpy ml tensorflow pytorch not libraries anaconda lets does framework tensor static temporaries popular install python torch fitting graphs frameworks default conda graph support gpu different dynamic note differentiation bash mlwork ipykernel using should systems source very org rather built model data avoid numba gpus functions computation quick couple automatic while channel set linux activate m user name picknamehere deactivate theano www directly learn debug still quite production chainer possible cupy performance just covering thats sample distribution parameters metric minimize usually instead runs great calculations its basic ndarray bit tensors math though run inside session fast tricky week objectives cover

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Static Computation Graphs</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Week-14-day-1:-Static-and-Dynamic-Computation-Graphs">Week 14 day 1: Static and Dynamic Computation Graphs<a class="anchor-link" href="#Week-14-day-1:-Static-and-Dynamic-Computation-Graphs"> </a></h1><h2 id="Objectives:">Objectives:<a class="anchor-link" href="#Objectives:"> </a></h2><ul>
<li>Quick note about Numpy temporaries</li>
<li>Cover a couple of common ML frameworks</li>
<li>Study automatic differentiation methods </li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While Google's TensorFlow and FaceBook's PyTorch are popular libraries, and (with some caveats) are available in the default Anaconda channel, they are not installed by default. Let's look at the many libraries briefly, then we will install and play with those two.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2><p>Let's set up a Conda environment with the necessary libraries. While Anaconda does have PyTorch, it only has it for Linux, so let's add the <code>pytorch</code> channel and install <code>pytorch</code> from there.</p>
<div class="highlight"><pre><span></span>conda create -n mlwork <span class="nv">python</span><span class="o">==</span><span class="m">3</span>.6 anaconda ipykernel tensorflow pytorch -c pytorch
</pre></div>
<p>You can replace the metapackage <code>anaconda</code> with the list of packages you will be using. If you are on OSC, logging out then back in should be enough to set this up for Jupyter.  On other systems using the latest Anaconda:</p>
<div class="highlight"><pre><span></span>conda activate mlwork
python -m ipykernel install --user --name &lt;pick_name_here&gt;
conda deactivate
</pre></div>
<p>Or, an older Anaconda:</p>
<div class="highlight"><pre><span></span><span class="nb">source</span> activate mlwork
python -m ipykernel install --user --name &lt;pick_name_here&gt;
<span class="nb">source</span> deactivate
</pre></div>
<p>(Skip <code>source</code> on Windows)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ML-Libraries">ML Libraries<a class="anchor-link" href="#ML-Libraries"> </a></h2><ul>
<li><a href="http://deeplearning.net/software/theano/"><strong>Theano</strong></a>: A research project, one of the very first static graph systems. Killed by newer frameworks except for use in MC3.</li>
<li><a href="https://www.tensorflow.org"><strong>Tensorflow</strong></a>: Powers Android's ML. Rapidly became most popular framework. Often used as a backend to a framework rather than directly (the popular Keras framework is now built in). Shifting from static graph to dynamic graph as default in 2.0 to look more like PyTorch (easier to learn and debug). Still no Python 3.7 support, so use Python 3.6.</li>
<li><a href="https://pytorch.org"><strong>PyTorch</strong></a>:  Comes from the old Lua-based Torch. Very easy to debug. Sort of similar but not quite to Numpy. Still heading to version 1.0, which will support production use (Facebook currently has Caffe, which is production ready). Very popular for such a young library. Has built-in ML framework. <em>Amazing</em> documentation. Best way to learn TensorFlow?!</li>
<li><a href="https://chainer.org"><strong>Chainer</strong></a>: The original basis of PyTorch's design. Stays as close as possible to Numpy (and CuPy was developed to power the GPU side of this framework).</li>
<li><a href="https://www.microsoft.com/en-us/cognitive-toolkit/"><strong>CNTK</strong></a>: Microsoft's offering, specializes in natural language processing.</li>
</ul>
<p>All the libraries have CPU/GPU support, decent performance, etc. There are lots more; these are just the most popular.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="ML-libraries-without-ML?">ML libraries without ML?<a class="anchor-link" href="#ML-libraries-without-ML?"> </a></h2><p>You may have noticed that I'm covering ML libraries <em>before</em> covering ML. That's because we have already covered fitting, and ML is mostly fitting. Let's review fitting:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>You have a sample of an underlying distribution</li>
<li>You build a model of that distribution</li>
<li>There are (many) parameters that describe the model</li>
<li>We select a metric that compares the model to the data</li>
<li>We want to minimize the parameters to give a good description of the data</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What's different in ML? We usually use a different metric instead of NLL, the models are larger but make of simpler parts. That's about it! So how can we improve fitting?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Automatic-Differentiation">Automatic Differentiation<a class="anchor-link" href="#Automatic-Differentiation"> </a></h2><p>Why use a framework instead of just writing plain Numpy? A few possible reasons:</p>
<ul>
<li><strong>Avoid temporaries</strong>: Numpy takes extra memory and time with temporaries. It would be nice to avoid them (note: Numba does this)</li>
<li><strong>Performance</strong>: Numpy runs separate bits of compiled code for each calculation. Some of these could be combined (note: Numba does this)</li>
<li><strong>GPU</strong>: Many systems have GPUs, and GPUs are great for massive but "simple" calculations. Numpy does not directly support GPUs (but CuPy, Numba, and others do).</li>
<li><strong>Differentiation</strong>: You can avoid a lot of calculations if you can get gradients easily! This is a big deal most ML frameworks.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Quick-aside:-Temporaries">Quick aside: Temporaries<a class="anchor-link" href="#Quick-aside:-Temporaries"> </a></h2><p>Numpy classically had issues with temporaries. It's much better now (at least on Numpy 1.13+ on Linux and macOS)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1_000_000</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">s</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">c</span> <span class="c1"># How many arrays are in memory? (classic: 5, 1.13+: 4 on some systems)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">ab</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">ab</span> <span class="o">+</span> <span class="n">c</span> <span class="c1"># Right here, how many arrays are in memory? (5)</span>
<span class="k">del</span> <span class="n">ab</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it
<span class="n">s</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">s</span> <span class="o">+=</span> <span class="n">c</span> <span class="c1"># (4)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Depending on your system, the first time should look like one of the other two times.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fitting-revisit">Fitting revisit<a class="anchor-link" href="#Fitting-revisit"> </a></h2><p>Let's make a small sample of data to fit.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100_000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">μ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dist</span><span class="p">):</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">params</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gaussian</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">minimize</span><span class="p">(</span><span class="n">nll</span><span class="p">,</span> <span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dist</span><span class="p">,))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dynamic-graphs:-PyTorch">Dynamic graphs: PyTorch<a class="anchor-link" href="#Dynamic-graphs:-PyTorch"> </a></h3><p>When converting to Torch, let's notice a couple of quirks compared to Numpy:</p>
<ul>
<li>The basic "ndarray" in ML frameworks is usually called a tensor. No, it is not a true mathematical tensor.</li>
<li>Torch loves 32-bit floats. You'll need to request 64 for every tensor.</li>
<li>Use the function <code>tensor</code> to make Tensors, not the constructor <code>Tensor</code></li>
<li>Use math functions from torch rather than Numpy. Numpy 1.13+ has the ability to call a custom library's functions, but Torch does not (yet?) use it.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">tdist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tmean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tsigma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">tgaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">μ</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">σ</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">result</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tgaussian</span><span class="p">(</span><span class="n">tdist</span><span class="p">,</span> <span class="n">tmean</span><span class="p">,</span> <span class="n">tsigma</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmean</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">tsigma</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Unfortunately, this is not trivial to put into minimize, since the autograd requires "result", and is built once each time this runs.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Static-Graphs:-TensorFlow">Static Graphs: TensorFlow<a class="anchor-link" href="#Static-Graphs:-TensorFlow"> </a></h3><p>TensorFlow does not make any pretense about looking or acting like Numpy.</p>
<ul>
<li>The basic "ndarray" is a Tensor again. Though you have customized tensors for different uses:  placeholders, constants, and more.</li>
<li>Use math functions from TensorFlow rather than Numpy. It's way too different; you are simply "scheduling" an operation, not making one.</li>
<li>TensorFlow has an "Eager Evaluation" mode that acts like PyTorch, and will become the "default" in 2.0. We'll stay with static graphs at the moment, though.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Make the distribution a constant Tensor; </span>
<span class="c1"># it does not change in iterations so TensorFlow can optimize for that.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>

<span class="c1"># Make placeholders for values we are going to &quot;feed&quot; in</span>
<span class="c1"># (0D tensor == scalar)</span>
<span class="n">tf_mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">tf_sigma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># tf_gaussian is a Tensor graph (like a function) can can compute this expression!</span>
<span class="n">tf_gaussian</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">tf_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">tf_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">tf_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This is still just a &quot;description of what to do&quot;, no computation has been done yet</span>
<span class="n">tf_nll</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf_gaussian</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># We can compute symbolic derivatives with the graph, as well</span>
<span class="n">tf_grads</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">tf_nll</span><span class="p">,</span> <span class="p">[</span><span class="n">tf_mean</span><span class="p">,</span> <span class="n">tf_sigma</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

    <span class="n">loss_value</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf_nll</span><span class="p">,</span>
                          <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">tf_mean</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
                                     <span class="n">tf_sigma</span><span class="p">:</span><span class="mf">0.5</span><span class="p">})</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf_grads</span><span class="p">,</span>
                     <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">tf_mean</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
                                <span class="n">tf_sigma</span><span class="p">:</span><span class="mf">0.5</span><span class="p">})</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notes:</p>
<ul>
<li>You have to run inside a session. Sessions are slow to start/stop, so loops should be inside the session (<code>sess.run</code> is fast).</li>
<li>The actual computation is quite fast, and can happen on the GPU.</li>
<li>TensorFlow is a bit verbose and tricky to setup, but can be amazingly clear.</li>
<li>(Not shown) TensorFlow has great graph visualization tools (TensorBoard) (using them is tricky)</li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    